{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **SOW-MKI49-2019-SEM1-V: NeurIPS**\n",
    "\n",
    "# Project: Skeleton Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeurosmashAgent:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def step(self, info, reward, state):\n",
    "        # return 0 # no action\n",
    "        # return 1 # left action\n",
    "        # return 2 # right action\n",
    "        # return 3 # built-in random action\n",
    "        return 4 # built-in ai action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import socket\n",
    "import struct\n",
    "\n",
    "class NeurosmashEnvironment:\n",
    "    def __init__(self, ip = \"127.0.0.1\", port = 13000):\n",
    "        self.client = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        self.ip     = ip\n",
    "        self.port   = port\n",
    "\n",
    "        self.client.connect((ip, port))\n",
    "\n",
    "    def init(self):\n",
    "        self._send(0, 1)\n",
    "        return self._receive()\n",
    "\n",
    "    def step(self, action):\n",
    "        self._send(action, 2)\n",
    "        return self._receive()\n",
    "\n",
    "    def _receive(self):\n",
    "        data   = self.client.recv(1 + 1 + 256 * 256 * 3)\n",
    "        info   = data[0]\n",
    "        reward = data[1]\n",
    "        state  = [data[i] for i in range(2, 196610)]\n",
    "        return info, reward, state\n",
    "\n",
    "    def _send(self, action, transition):\n",
    "        self.client.send(bytes([action, transition]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent       = NeurosmashAgent()\n",
    "environment = NeurosmashEnvironment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "info, reward, state = environment.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def dense_nn(input_sh, output_sh):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(16, (3, 3), activation='relu', input_shape=input_sh))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(16, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(16, (3, 3), activation='relu'))\n",
    "    model.add(layers.Dense(output_sh, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dql():\n",
    "    input_shape = (256, 256, 3)\n",
    "    actions = 2\n",
    "    info, reward, state = environment.init()\n",
    "    epsilon = 1\n",
    "    replay_buffer = []\n",
    "    Q_network = dense_nn(input_shape, actions)\n",
    "    Q_head_network = dense_nn(input_shape, actions)\n",
    "    action = 0\n",
    "    \n",
    "    # fill replay buffer\n",
    "    replay_size = 50\n",
    "    for i in range(replay_size):\n",
    "        state_old = state\n",
    "        r = random.uniform(0, 1)\n",
    "        if r < epsilon:\n",
    "            rs = random.uniform(0, 1)\n",
    "            if rs < 0.5:\n",
    "                action = 1\n",
    "\n",
    "            else:\n",
    "                action = 2\n",
    "        else:\n",
    "            p = network.predict(state)\n",
    "            action = np.argmax(p)\n",
    "        print(action)\n",
    "        info, reward, state = environment.step(action)\n",
    "        replay_buffer.append([state_old, action, reward, state])\n",
    "        \n",
    "    #random minibatch\n",
    "    indices = np.random.randint(0, high=replay_size-1, size=int(replay_size/5))\n",
    "    minibatch = replay_buffer[indices]\n",
    "    print(len(minibatch))\n",
    "    return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dql()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
