{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **SOW-MKI49-2019-SEM1-V: NeurIPS**\n",
    "#### Project: Neurosmash\n",
    "\n",
    "This is the info document on the (updated*) Neurosmash environment that you will be using for your project. It contains background info and skeleton code to get you started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project\n",
    "\n",
    "In the next 4 + 1 weeks, you will be working exclusively on your project in the practicals. The goal is to take what has been discussed in class and what you have already worked on in the earlier practicals, and apply them on a RL problem in a novel environment. Note that while the earlier practicals were intended to give you the opportunity to gain experience with various RL topics and were not graded, your project will constitute 50% of your final grade.\n",
    "\n",
    "Your project grade will be based on the following components:\n",
    "- Online demonstration\n",
    "- Source code\n",
    "- Written report\n",
    "\n",
    "These components will be evaluated based on performance, creativity, elegance, rigor and plausibility.\n",
    "\n",
    "While you can use the material from earlier practicals (e.g., REINFORCE, DQN, etc.) as a boilerplate, you are also free to take any other approach be it imitation learning or world models for your project.\n",
    "\n",
    "In addition to the practical sessions, we will provide additional support in the coming four weeks. You can email any of us to set up an appointment for discussing your project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "\n",
    "Briefly, there are two agents: Red and Blue. Red is controlled by you. Blue is controlled by the environment \"AI\".* Both agents always run forward with a speed of 3.5 m/s*. If one of them gets within the reach of the other (a frontal sphere with 0.5 m radius), it gets pushed away automatically with a speed of 3.5 m/s. The only thing that the agents can do is to turn left or right with an angular speed of 180 degrees/s. This means that there are three possible discrete actions that your agent can take every step: Turn nowhere, turn left and turn right. For convenience, there is also a fourth built-in action which turns left or right with uniform probability. An episode begins when you reset the environment and ends when one of the agents fall off the platform. At the end of the episode, the winning agent gets a reward of 10 while the other gets nothing. Therefore, your goal is to train an agent who can maximize its reward by pushing the other agent off the platform or making it fall off the platform by itself.\n",
    "\n",
    "* None that all times are simulation time. That is, 0.02 s per step when timescale is set to one.\n",
    "\n",
    "* Basically, Blue is artificial but not really intelligent. What it does is that every 0.5 s, it updates its destination to the current position of Red plus some random variation (a surrounding circle with a radius of 1.75 m) and smoothly turns to that position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Updates\n",
    "\n",
    "* There has been several small changes made to the lite version based on your feedback. Most notable ones are:\n",
    "- Bugs have (hopefully) been completely eliminated. Any remaining bug/glitch that your agent \"learns\" exploit will be considered fair game.\n",
    "\n",
    "- TCP/IP interface has been made more robust (you can now stop and start the simulation with the gui. no need to quit and rerun the environment anymore to reset it if something goes wrong.)\n",
    "- Animations/graphics have been updated (you can now tell what is going on more easily. agents actually fall down, etc.)\n",
    "- Last but not least, size and timescale settings have been added (you can now change the resolution and the speed of the environment to make the simulation run faster). In other words:\n",
    "\n",
    "Size => This is the size of the texture that the environment is rendered. This is set to 784 by default, which will result in a crisp image but slow speed. You can change the size to a value that works well for your environment should not go too low.\n",
    "\n",
    "Timescale => This is the simulation speed of the environment. This is set to 1 by default. Setting it to n will make the simulation n times faster. In other words, less (if n < 1) or more (if n > 1) simulation time will pass per step. You might want to increase this value to around 10 if you cannot train your models fast enough so that they can sample more states in a shorter number of steps at the expense of precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc. FAQs\n",
    "\n",
    "Q: Can we get HCP access?  \n",
    "A: I will try provide access to the AI HPC cluster if you require additional resources. If this is something that you would like, please contact me. Note however that you should use the cluster for training your final model and not development.\n",
    "\n",
    "Q: Will the environment code be shared?  \n",
    "A: Yes. I will share the entire unityproject at the end of the course (but without the 3D agent models).\n",
    "\n",
    "Q: Is there a environment version that can be played with a mouse/keyboard?  \n",
    "A: No but I will make one and update Brightspace when I have some free time.\n",
    "\n",
    "Q: I found a bug/glitch. What should I do?  \n",
    "A: Please let me know and I will fix it. Do note however that any updates from this point on will be optional to adopt. That is, you can keep working on the current environment if you so wish or think that updating will disadvantage you in any way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skeleton code\n",
    "\n",
    "- You should first add the Neurosmash file to your working directory or Python path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Neurosmash\n",
    "\n",
    "# These are the default environment arguments. They must be the same as the values that are set in the environment GUI.\n",
    "ip         = \"127.0.0.1\" # Ip address that the TCP/IP interface listens to\n",
    "port       = 13000       # Port number that the TCP/IP interface listens to\n",
    "size       = 768         # Please check the Updates section above for more details\n",
    "timescale  = 1           # Please check the Updates section above for more details\n",
    "\n",
    "agent = Neurosmash.Agent() # This is an example agent.\n",
    "                           # It has a step function, which gets reward/state as arguments and returns an action.\n",
    "                           # Right now, it always outputs a random action (3) regardless of reward/state.\n",
    "                           # The real agent should output one of the following three actions:\n",
    "                           # none (0), left (1) and right (2)\n",
    "\n",
    "env = Neurosmash.Environment() # This is the main environment.\n",
    "                                       # It has a reset function, which is used to reset the environment before episodes.\n",
    "                                       # It also has a step function, which is used to which steps one time point\n",
    "                                       # It gets an action (as defined above) as input and outputs the following:\n",
    "                                       # end (true if the episode has ended, false otherwise)\n",
    "                                       # reward (10 if won, 0 otherwise)\n",
    "                                       # state (flattened size x size x 3 vector of pixel values)\n",
    "                                       # The state can be converted into an image as follows:\n",
    "                                       # image = np.array(state, \"uint8\").reshape(size, size, 3)\n",
    "                                       # You can also use to Neurosmash.Environment.state2image(state) function which returns\n",
    "                                       # the state as a PIL image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'environment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-9f729e8dfffb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# The following steps through an entire episode from start to finish with random actions (by default)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mend\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'environment' is not defined"
     ]
    }
   ],
   "source": [
    "# The following steps through an entire episode from start to finish with random actions (by default)\n",
    "\n",
    "end, reward, state = env.reset()\n",
    "\n",
    "while (end == 0):\n",
    "    action = agent.step(end, reward, state)\n",
    "    end, reward, state = env.step(action)\n",
    "\n",
    "# Let's run it a few more steps so that the things have time to settle down\n",
    "\n",
    "for i in range(100):\n",
    "    action = agent.step(end, reward, state)\n",
    "    end, reward, state = env.step(action)\n",
    "\n",
    "#environment.state2image(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also do it step by step while displaying the state\n",
    "\n",
    "end, reward, state = env.reset()\n",
    "\n",
    "#environment.state2image(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(3, 8, kernel_size=4, stride=1, padding=1)\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        self.conv2 = torch.nn.Conv2d(8, 16, kernel_size=4, stride=1, padding=1)\n",
    "        \n",
    "        self.conv3 = torch.nn.Conv2d(16, 32, kernel_size=4, stride=1, padding=1)\n",
    "        \n",
    "        self.conv4 = torch.nn.Conv2d(32, 64, kernel_size=4, stride=1, padding=1)\n",
    "        \n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.dense = nn.Linear(64*47*47, 3)\n",
    "        # Output layer, 10 units - one for each digit\n",
    "        self.output = nn.Linear(3, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of our operations\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool(x)\n",
    "        print(x.shape)\n",
    "        x = self.dense(x.flatten())\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-6a90ecf1a670>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# state = 768*768*3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#state_1 = environment.state2image(state)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#print(type(np.array(environment.state2image(state))[1][1][1]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "action = agent.step(end, reward, state)\n",
    "end, reward, state = env.step(action)\n",
    "# state = 768*768*3\n",
    "#state_1 = environment.state2image(state)\n",
    "#print(type(np.array(environment.state2image(state))[1][1][1]))\n",
    "data = [np.swapaxes(np.array(environment.state2image(state), dtype='d'),0,2)]\n",
    "x = torch.tensor(data, dtype=torch.double)\n",
    "# print(type(environment.state2image(state)))\n",
    "# print(type(np.array(environment.state2image(state), dtype='d')[0][0][0]))\n",
    "\n",
    "# print(x)\n",
    "my_network = Network().double()\n",
    "my_network.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
