{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we will continue working on the inverted pendulum environment. However, the goal of this lab will be to implement one of the RL agents discussed in this week's lecture. Don't forget to download the new version of the environment under course contents. In the rest of this notebook, a Chainer skeleton code (with missing functionality) for the reinforce agent is provided. You can either work on finalizing the implementation of the reinforce agent or implement a different model of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from chainer import Chain\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "from chainer.optimizers import Adam\n",
    "from chainer import Variable\n",
    "import random\n",
    "import socket\n",
    "import struct\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same environment as last week\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, ip = \"127.0.0.1\", port = 13000):\n",
    "        self.client = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        self.ip     = ip\n",
    "        self.port   = port\n",
    "\n",
    "        self.client.connect((ip, port))\n",
    "\n",
    "    def reset(self):\n",
    "        self._send(0, 0)\n",
    "        return self._receive()\n",
    "\n",
    "    def step(self, action):\n",
    "        self._send(action, 1)\n",
    "        return self._receive()\n",
    "\n",
    "    def _receive(self):\n",
    "        data = self.client.recv(19)\n",
    "        reward = data[0]\n",
    "        state = [struct.unpack(\"@f\", data[1 + i * 4: 5 + i * 4])[0] for i in range(4)]\n",
    "        status = [data[17], data[18]]\n",
    "        return reward, state, status\n",
    "\n",
    "    def _send(self, action, command):\n",
    "        self.client.send(bytes([action, command]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a baseline agent which just emits random actions.\n",
    "\n",
    "\n",
    "class RandomAgent:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def step(self, reward, state):\n",
    "        return random.randint(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the agent within the environment. (don't forget to start the environment)\n",
    "if you want to see the agent, also don't forget to enable the camera in the GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = RandomAgent()\n",
    "environment = Environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'episode_count = 10\\nR0 = np.zeros(episode_count)\\n\\nfor i in range(1000):\\n    reward, state, status = environment.reset()\\n\\n    while (status[0] == 0):\\n        action = agent.step(reward, state)\\n        reward, state, status = environment.step(action)\\n        R0[i] += reward\\n        '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"episode_count = 10\n",
    "R0 = np.zeros(episode_count)\n",
    "\n",
    "for i in range(1000):\n",
    "    reward, state, status = environment.reset()\n",
    "\n",
    "    while (status[0] == 0):\n",
    "        action = agent.step(reward, state)\n",
    "        reward, state, status = environment.step(action)\n",
    "        R0[i] += reward\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create the REINFORCE agent. We assume that the policy is computed using an MLP with a softmax output.\n",
    "\n",
    "class MLP(Chain):\n",
    "    \"\"\"Multilayer perceptron\"\"\"\n",
    "\n",
    "    def __init__(self, n_output=1, n_hidden=5):\n",
    "        super(MLP, self).__init__(l1=L.Linear(None, n_hidden), l2=L.Linear(n_hidden, n_output))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.l2(F.relu(self.l1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A skeleton for the REINFORCEAgent is given. Implement the compute_loss and compute_score functions.\n",
    "\n",
    "class REINFORCEAgent(object):\n",
    "    \"\"\"Agent trained using REINFORCE\"\"\"\n",
    "\n",
    "    def __init__(self, model, optimizer=Adam()):\n",
    "        self.model = model\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.optimizer.setup(self.model)\n",
    "\n",
    "        # monitor score and reward\n",
    "        self.rewards = []\n",
    "        self.scores = []\n",
    "\n",
    "\n",
    "    def step(self, reward, state):\n",
    "\n",
    "        # linear outputs reflecting the log action probabilities and the value\n",
    "        policy = self.model(Variable(np.atleast_2d(np.asarray(state, 'float32'))))\n",
    "\n",
    "        # generate action according to policy\n",
    "        p = F.softmax(policy).data\n",
    "\n",
    "        # normalize p in case tiny floating precision problems occur\n",
    "        row_sums = p.sum(axis=1)\n",
    "        p /= row_sums[:, np.newaxis]\n",
    "\n",
    "        action = np.asarray([np.random.choice(p.shape[1], None, True, p[0])])\n",
    "\n",
    "        return action, policy\n",
    "\n",
    "\n",
    "    def compute_loss(self):\n",
    "        \"\"\"\n",
    "        Return loss for this episode based on computed scores and accumulated rewards\n",
    "        \"\"\"       \n",
    "        \n",
    "        return Variable(sum([sum(self.scores[:p1])*r1 for p1,r1 in zip(np.arange(1,len(self.rewards)+1),self.rewards)]))\n",
    "\n",
    "    def compute_score(self, action, policy):\n",
    "        \"\"\"\n",
    "        Computes score\n",
    "\n",
    "        Args:\n",
    "            action (int):\n",
    "            policy:\n",
    "\n",
    "        Returns:\n",
    "            score\n",
    "        \"\"\"\n",
    "        return F.log(policy[0][action[0]]) + sum(self.scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we run the REINFORCE agent within the environment. Note that we update the agent after each episode for simplicity.\n",
    "# First, we should restart the server from the GUI\n",
    "\n",
    "environment = Environment()\n",
    "network = MLP(n_output=2, n_hidden=3)\n",
    "agent = REINFORCEAgent(network, optimizer=Adam())\n",
    "\n",
    "episode_count = 1000\n",
    "\n",
    "    \n",
    "R = np.zeros(episode_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1000 [00:00<?, ?it/s]C:\\Users\\Arjan\\Anaconda3\\lib\\site-packages\\chainer\\functions\\math\\exponential.py:59: RuntimeWarning: divide by zero encountered in log\n",
      "  return utils.force_array(numpy.log(x[0])),\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "numpy.ndarray or cuda.ndarray are expected.\nActual: <class 'chainer.variable.Variable'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-1931ae287e02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcleargrads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-b58604c3af10>\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \"\"\"       \n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mp1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mr1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\chainer\\variable.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, **kwargs)\u001b[0m\n\u001b[0;32m    503\u001b[0m             msg = '''numpy.ndarray or cuda.ndarray are expected.\n\u001b[0;32m    504\u001b[0m Actual: {0}'''.format(type(data))\n\u001b[1;32m--> 505\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    506\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    507\u001b[0m         \u001b[1;31m# Use a list as a data structure to hold the data array indirectly to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: numpy.ndarray or cuda.ndarray are expected.\nActual: <class 'chainer.variable.Variable'>"
     ]
    }
   ],
   "source": [
    "for i in tqdm.trange(episode_count):\n",
    "\n",
    "    reward, state, status = environment.reset()\n",
    "\n",
    "    loss = 0\n",
    "    while True:\n",
    "\n",
    "        action, policy = agent.step(reward, state)\n",
    "\n",
    "        reward, state, status = environment.step(action[0])\n",
    "\n",
    "        # get reward associated with taking the previous action in the previous state\n",
    "        agent.rewards.append(reward)\n",
    "        R[i] += reward\n",
    "\n",
    "        # recompute score function: grad_theta log pi_theta (s_t, a_t)\n",
    "        agent.scores.append(agent.compute_score(action, policy))\n",
    "\n",
    "        # we learn at the end of each episode\n",
    "        if status[0] == 1:\n",
    "            \n",
    "            loss += agent.compute_loss()\n",
    "            \n",
    "            agent.model.cleargrads()\n",
    "            loss.backward()\n",
    "            loss.unchain_backward()\n",
    "            agent.optimizer.update()\n",
    "\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and we finally plot the accumulated reward per episode\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(R0))\n",
    "plt.plot(np.cumsum(R))\n",
    "plt.legend(['Random', 'REINFORCE'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
